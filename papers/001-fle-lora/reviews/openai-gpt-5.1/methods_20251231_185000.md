/Users/palaitis/Development/piensa/tools/bin/review-methods:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running methods reviewer (openai/gpt-5.1)...
**Methods & Reproducibility Checklist**

---

### 1. Experimental Design

1.1 Factorial structure (4×4×2, 32 conditions, 50 trials each, temperature=0.7)  
**Status: Clear**  
**Note:** Design is well specified and sufficient to reconstruct the condition matrix and nominal sample sizes.

1.2 Task description and prompts (Asian Disease; gain/loss; 4 languages)  
**Status: Needs clarification**  
**Note (risk):**  
- Full English, Spanish, Hebrew, and Chinese gain frames are shown, but loss frames are only given for English. It is not explicit whether Spanish/HE/ZH loss prompts are exact translations of Costa et al. (2014), or whether any edits/normalization were made.  
- It is not stated whether prompts included any surrounding system / role / chat-template text (e.g., Mistral’s chat format). Small changes here can alter instruction-following and unclear rates.

1.3 Trial independence and ordering  
**Status: Missing**  
**Note (risk):**  
- No description of whether prompts in different conditions were intermixed or run in blocks, whether conversation state was reset between trials, or whether any prior context from earlier prompts persisted.  
- Without explicit “single-turn, stateless” confirmation, it is ambiguous whether drift or cross-condition contamination could affect response patterns.

1.4 Frame counterbalancing / randomization  
**Status: Clear**  
**Note:** Frames are handled by separate conditions (gain vs loss, 50 trials each), not by randomization at trial level; this is reproducible as reported.

---

### 2. Model and Adapter Training

2.1 Base model specification  
**Status: Needs clarification**  
**Note (risk):**  
- “Mistral-7B-Instruct-v0.3 (4-bit quantized via MLX)” is given, but the quantization scheme (e.g., NF4/int4, group size, symmetric/asymmetric), MLX version, and whether quantization was applied during LoRA training or only at inference are not specified.  
- This affects reproducibility of adapter behavior and instruction-following degradation.

2.2 LoRA configuration (rank, alpha, target layers, LR, iterations, batch)  
**Status: Needs clarification**  
**Note (risk):**  
- Rank, alpha, “target layers: 16”, LR, “training iterations: 100”, batch=2 are given, but:  
  - It is not specified which *kinds* of layers were adapted (e.g., all attention projection matrices, MLPs, embeddings?) nor which 16 layers in a 7B transformer (e.g., every Nth layer, only encoder blocks, etc.).  
  - No mention of optimizer type (AdamW, Adam, etc.), weight decay, gradient clipping, LR schedule, or lora_dropout. These are necessary to faithfully replicate training.  
  - “100 training iterations” with 5,000 samples and batch size 2 implies only 200 examples seen if one iteration = one optimizer step; this seems unusually low and should be made explicit (iterations vs epochs).  
  - It is not stated whether the base model weights were fully frozen (standard for LoRA but should be explicit for reproducibility).

2.3 Adapter training precision and pipeline  
**Status: Missing**  
**Note (risk):**  
- It is unclear whether LoRA training was done on the quantized model (QLoRA-style) or on a full-precision model later quantized for inference. This can materially change adapter behavior and hence unclear rates.  
- No indication of hardware, framework versions, or random seed for training.

2.4 Training data sources and preprocessing  
**Status: Needs clarification**  
**Note (risk):**  
- Data sources are named and sample counts plus final val loss given, which is good. However:  
  - No version/commit hashes of the datasets (e.g., for Stanford Alpaca and somos-alpaca-es).  
  - For HE (GPT‑4o translations), there is no specification of the translation prompt, temperature, or post-processing (e.g., handling format artifacts, right-to-left issues).  
  - No description of train/validation split method, shuffle seed, or whether any filtering (e.g., length limits, de-duplication) was applied.  
- These omissions make it difficult to reproduce the *exact* adapter behavior, especially for HE.

2.5 Adapter selection and activation at inference  
**Status: Missing**  
**Note (risk):**  
- It is not stated how adapters were loaded and applied at inference (e.g., via PEFT with a specific scaling, whether only a single adapter was active at a time, whether any residual base instruction LoRA or other adapters were also loaded).  
- This ambiguity affects both cross-lingual interference and instruction-following degradation, which are central to the paper.

---

### 3. Inference Parameters & Sampling

3.1 Sampling hyperparameters  
**Status: Needs clarification**  
**Note (risk):**  
- Only temperature=0.7 is given. No mention of top_p, top_k, max_new_tokens, repetition penalties, or stop sequences.  
- Different defaults across toolchains (MLX or other inference frameworks) will yield different token distributions, so choice probabilities and unclear rates may not replicate exactly.

3.2 Random seed use at inference  
**Status: Missing**  
**Note (risk):**  
- There is no information on seeding: whether a fixed seed was used per condition, per trial, or whether library defaults were used.  
- For 50 stochastic trials per cell, seed control is necessary to exactly reproduce choice proportions, or at least to know that exact replication is not expected.

3.3 Context formatting / chat template  
**Status: Needs clarification**  
**Note (risk):**  
- Mistral-Instruct models typically expect a chat template; the paper does not specify whether prompts were passed as raw text or wrapped in a specific template (e.g., `<s>[INST] ... [/INST]`).  
- Such differences can materially alter instruction-following and thus the unclear rates; for replication, the exact formatting must be given.

3.4 Response length and truncation  
**Status: Missing**  
**Note (risk):**  
- No max token length, early-stopping criteria, or post-processing are specified.  
- Some “unclear” responses are long explanations; if a different max_new_tokens or EOS handling is used, the rate of format non-compliance could change substantially.

---

### 4. Response Classification and Metrics

4.1 Formal classification rules (A/B/Unclear)  
**Status: Needs clarification**  
**Note (risk):**  
- The stated rules:  
  - A: “contains only ‘A’ or starts with ‘A’ with no mention of ‘B’”  
  - B: analogous  
  - Unclear: “both A and B mentioned, or neither mentioned, or verbose explanation”.  
- Ambiguities:  
  - Are responses like “I choose A” (not starting with ‘A’) or “Option A.” treated as A or Unclear?  
  - Are responses with lowercase/Unicode variants (“a”, “Ａ”) normalized?  
  - Are non-Latin equivalents (e.g., answers in Hebrew or Chinese script) ever used, or is it guaranteed that the model always uses Latin ‘A’/‘B’?  
- These details affect reproducibility of classification scripts.

4.2 Consistency between written rules and actual handling of “language leakage”  
**Status: Needs clarification**  
**Note (risk):**  
- Section 4 describes “Type 2: Language Leakage” examples that begin with “A.” or “B.” followed by an explanation in the same language. According to the stated rule (“starts with ‘A’ with no mention of ‘B’”), these should be coded as clear A/B responses, yet they are presented as examples of *unclear* responses.  
- This inconsistency suggests that additional, undocumented criteria (e.g., any additional text beyond the single character) may have been used to mark some such responses as unclear.  
- Without explicit coding rules that match the actual implementation, reproducing Table 1’s “unclear” rates is not possible.

4.3 Automation vs. manual coding  
**Status: Needs clarification**  
**Note (risk):**  
- It is not stated whether classification was done by a deterministic script (e.g., regex over raw outputs) or by manual/part-manual annotation.  
- Section 4 says “We manually examined unclear responses”, but it remains unclear whether manual judgment was used to *assign* unclear labels or only to build the taxonomy.  
- If any manual labeling was used, inter-rater reliability or annotation protocol is needed for reproducibility.

4.4 Definition of P(A|frame) and Δ  
**Status: Needs clarification**  
**Note (risk):**  
- Δ is defined as P(A|gain) − P(A|loss), but it is not explicitly stated whether P(A|gain) is computed conditional on *clear* responses only (i.e., denominator = # clear A + # clear B), or whether unclear responses remain in the denominator.  
- The text strongly implies that unclear responses are excluded when computing P(A|frame), but this should be explicit and formulaic to make replication unambiguous.

4.5 Handling of very high unclear rates  
**Status: Needs clarification**  
**Note (risk):**  
- The paper warns that framing estimates with >50% unclear “should be interpreted with caution,” but does not specify any *analytic* handling rule:  
  - Were such conditions included in any aggregate analyses?  
  - Were sensitivity analyses run (e.g., treating unclear as missing vs as equal split vs as A/B)?  
- While not strictly required for replication of raw numbers, clear rules help future researchers interpret which conditions are considered methodologically usable.

4.6 Handling of other failure modes (timeouts, API errors)  
**Status: Missing**  
**Note (risk):**  
- There is no mention of whether any generations failed for technical reasons (e.g., timeouts, errors) and how such trials were handled (re-run vs dropped).  
- This could affect actual N per cell and reproducibility of choice proportions if some runs were discarded.

---

### 5. Unclear-Rate Handling and Interpretation

5.1 Formal definition of “Unclear rate”  
**Status: Clear**  
**Note:** Defined as “proportion of responses not classifiable as clear A or B choices.” This is straightforward, given the classification rules.

5.2 Application across conditions and matrices (Table 1 and Figure 1)  
**Status: Clear**  
**Note:** Numbers for unclear(gain)/unclear(loss) per cell and aggregate per adapter/prompt are fully reported, enabling others to know which cells are heavily affected.

5.3 Link between unclear taxonomy and classification scheme  
**Status: Needs clarification**  
**Note (risk):**  
- Section 4’s taxonomy of unclear response types is informative but does not map directly onto explicit classification rules (e.g., how many unclear responses were Type 1 vs Type 2 vs Type 3 per condition).  
- For reproducibility and interpretability, a clearer mapping from observable string patterns to categories would help others reconstruct or extend the taxonomy in new models.

---

### 6. Data, Code, and Artifacts

6.1 Availability of prompts and raw responses  
**Status: Missing (in text)**  
**Note (risk):**  
- The paper does not state whether raw model outputs, trial-level data, or scripts are publicly available.  
- Given the sensitivity of results to subtle classification and prompting details, lack of data/code availability significantly hampers precise replication and meta-analysis of unclear-rate behavior.

6.2 Adapter artifacts  
**Status: Missing**  
**Note (risk):**  
- There is no indication whether the trained LoRA adapter weights (EN, ES, HE, ZH) are released, or if not, how they might be reconstructed (e.g., with fixed seeds and exact data versions).  
- Without access to these adapters, reproducing the exact reported matrix (especially the severe degradation from non-English adapters) is unlikely.

---

### 7. Summary of Key Interpretability Risks

- **Adapter training ambiguity:** Missing details on layer selection, optimizer, training regime, and quantization path make it hard to disentangle whether observed instruction-following degradation is due to language, training setup, or tooling differences.
- **Sampling configuration and seeding:** Incomplete description of sampling hyperparameters and seeding leaves trial-level stochastic behavior under-specified, limiting exact replication of P(A|frame) and unclear rates.
- **Classification-rule vs implementation mismatch:** The conflict between the written A/B/Unclear criteria and the handling of “language leakage” responses is the most serious reproducibility concern. Without exact, implementation-level rules, others cannot reliably recreate the unclear-rate statistics or their downstream impact on Δ.
- **Lack of artifact release:** Absent shared code/adapters, replication will at best approximate the findings; precise reconstruction of the evaluation failure profile is unlikely.

If these points are clarified—especially the exact classification implementation, sampling configuration, and adapter training procedure—a reader could more confidently re-run the experiment and know which cells are methodologically interpretable and which are dominated by evaluation failures.
