/Users/palaitis/Development/piensa/tools/bin/review-epistemic:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running epistemic reviewer (anthropic/claude-sonnet-4.5)...
# Epistemic Hygiene Review: "Piensa Twice" Paper

## Overreaching Claims Requiring Scope-Narrowing

### 1. Abstract - Mechanism Attribution

**Quoted:** "simulating L1" and "simulating L2"

**Why it overreaches:** The paper tested adapter-prompt language matching, not simulation of L1/L2 processing. "Simulating" implies the operationalization successfully captures the underlying mechanism, which the results contradict.

**Suggested rewrite:** "hypothesizing that matched adapter-prompt pairs would produce stronger framing effects than mismatched pairs (analogous to the L1/L2 distinction in human studies)"

---

### 2. Abstract - Unclear Response Characterization

**Quoted:** "This evaluation failure is distinct from the proxy failure above"

**Why it overreaches:** While the two findings are separable, calling high unclear rates an "evaluation failure" implies a known correct evaluation exists. The paper measured what it measured: response compliance rates.

**Suggested rewrite:** "This limitation in response interpretability is methodologically distinct from the negative finding on framing patterns"

---

### 3. Abstract - Comparative Claim About Costa et al.

**Quoted:** "It also raises a question applicable to the original Costa et al. study: what proportion of human participants gave unclear or invalid responses? The original paper does not report this."

**Why it overreaches:** This implies equivalence between LLM instruction-following failures and human response validity issues without justification. The mechanisms are not shown to be comparable.

**Suggested rewrite:** "Similar reporting of response validity rates in the original Costa et al. study would aid methodological comparison, though the underlying causes of unclear responses likely differ between human and LLM contexts."

---

### 4. Introduction (1.1) - Processing Claims

**Quoted:** "Unlike humans, LLMs do not have a developmental L1 acquired in childhood through emotional and social interaction."

**Why it overreaches:** While true, this implicitly claims knowledge about what constitutes "native processing" in LLMs without operationalizing it.

**Suggested rewrite:** "Unlike humans, LLMs do not have a developmental learning trajectory with early-acquired languages, making direct analogies to L1/L2 distinctions conceptually uncertain."

---

### 5. Introduction (1.1) - Proxy Assumption

**Quoted:** "Yet, the training data distribution is heavily skewed toward English, potentially creating an asymmetry in processing depth across languages."

**Why it overreaches:** "Processing depth" is not measured or defined operationally. This is speculation about internal mechanisms.

**Suggested rewrite:** "Yet, the training data distribution is heavily skewed toward English, which may create asymmetries in model behavior across languages, though the nature of such asymmetries remains unspecified."

---

### 6. Introduction (1.1) - Operational Proxy Language

**Quoted:** "differential processing associated with native versus foreign language use"

**Why it overreaches:** The study doesn't measure "processing" differences—it measures choice distributions and response validity.

**Suggested rewrite:** "behavioral differences in choice patterns associated with adapter-prompt language alignment"

---

### 7. Related Work (FLE subsection) - Generalization

**Quoted:** "human studies typically report choice distributions over included trials but do not systematically report rates of misunderstanding, invalid responses, or exclusions"

**Why it overreaches:** "Typically" and "systematically" are universal claims without systematic evidence. The paper cites one study (Costa et al.).

**Suggested rewrite:** "Costa et al. (2014) and several other studies we reviewed report choice distributions over included trials but do not report rates of misunderstanding, invalid responses, or exclusions"

---

### 8. Related Work (Multilingual LLMs) - Mechanism Claim

**Quoted:** "multilingual models often rely on internal normalization or translation mechanisms rather than maintaining fully independent language-specific reasoning pathways"

**Why it overreaches:** This describes internal mechanisms not directly observed. The cited work (Shaham et al., 2024) likely shows behavioral patterns, not verified internal mechanisms.

**Suggested rewrite:** "multilingual models often exhibit behavior consistent with internal normalization or translation processes rather than fully independent language-specific pathways, based on observed input-output patterns"

---

### 9. Related Work (Parameter-efficient fine-tuning) - Mechanism Attribution

**Quoted:** "language-specific adaptation can lead to interference, uneven cross-lingual transfer, or degradation of instruction-following behavior"

**Why it overreaches:** These are descriptive labels for observed patterns, not demonstrated causal mechanisms.

**Suggested rewrite:** "language-specific adaptation is associated with patterns consistent with interference, uneven cross-lingual transfer, or instruction-following degradation in observed outputs"

---

### 10. Contributions - Operationalization Status

**Quoted:** "A testable operationalization of L1/L2 processing in LLMs"

**Why it overreaches:** The results show it's *not* a valid operationalization of L1/L2 processing—it's a tested-and-failed operationalization attempt.

**Suggested rewrite:** "A concrete and falsifiable hypothesis about adapter-based language alignment as a potential proxy for L1/L2 processing differences"

---

### 11. Results Section 3.2 - Hypothesis Language

**Quoted:** "The FLE hypothesis predicts matched > mismatched framing effects."

**Why it overreaches:** The FLE hypothesis applies to humans. The *adapter-alignment* hypothesis predicts this pattern in this specific operationalization.

**Suggested rewrite:** "If adapter-prompt matching were a valid proxy for the L1/L2 distinction, we would predict matched > mismatched framing effects."

---

### 12. Analysis 4.3 - Mechanism Claims

**Quoted:** "language-specific LoRA training may disrupt the instruction-following behavior learned during the base model's instruction tuning phase"

**Why it overreaches:** "Disrupt" implies a causal mechanism. The observation is that unclear rates increased; the mechanism is not established.

**Suggested rewrite:** "language-specific LoRA training was associated with increased unclear response rates, suggesting possible interference with instruction-following capabilities from the base model"

---

### 13. Analysis 4.3 - Processing Attribution

**Quoted:** "the model appears to: 1. Lose format adherence... 2. Exhibit processing confusion... 3. Show language interference"

**Why it overreaches:** "Processing confusion" anthropomorphizes and attributes internal states. "Language interference" implies a mechanism.

**Suggested rewrite:** "the model's outputs exhibit: 1. Reduced format adherence... 2. References to tangentially related concepts... 3. Mixed-language responses"

---

### 14. Discussion 5.1 - Proxy Mapping Language

**Quoted:** "The L1/L2 mapping does not hold."

**Why it overreaches:** This implies a mapping was established to test. What was tested is whether adapter-matching produces the predicted pattern.

**Suggested rewrite:** "Adapter-prompt language matching did not produce the pattern predicted by the L1/L2 analogy."

---

### 15. Discussion 5.2 - Mechanism Explanation

**Quoted:** "One explanation for the FLE in humans involves differential processing fluency: L1 is hypothesized to be automatic and emotionally resonant, while L2 requires effort and creates distance."

**Why it overreaches:** This section correctly describes human theories but then says "Our LoRA approach assumed..." without clarifying this is about the operationalization, not the human mechanism.

**Suggested rewrite:** "In human studies, one explanation for the FLE involves differential processing fluency. Our operationalization attempted to instantiate an analogous distinction through adapter-prompt matching, but..."

---

### 16. Discussion 5.2 - Capability vs. Mechanism

**Quoted:** "LoRA modifies capabilities, not processing style"

**Why it overreaches:** "Processing style" is not operationally defined or measured.

**Suggested rewrite:** "LoRA fine-tuning may primarily affect output generation patterns in specific languages without creating the behavioral asymmetries predicted by the L1/L2 analogy"

---

### 17. Discussion 5.3 - Robustness Claim

**Quoted:** "the framing effect was robust"

**Why it overreaches:** "Robust" typically means across varied conditions/manipulations. Here it was observed in specific conditions with low unclear rates.

**Suggested rewrite:** "the framing effect was consistently present in conditions with interpretable response data"

---

### 18. Discussion 5.3 - Universal Negative

**Quoted:** "However, we cannot claim this holds for all 16 conditions because many had >50% unclear responses"

**Why it overreaches:** This is actually appropriately scoped—no revision needed. (Included for contrast.)

---

### 19. Discussion 5.5 - Alternative Mechanism Claims

**Quoted:** "Train on learner-quality text (beginner textbooks, non-native errors) to create genuinely disfluent processing"

**Why it overreaches:** "Genuinely disfluent processing" assumes this would create the mechanism, not just the training data type.

**Suggested rewrite:** "Train on learner-quality text to test whether outputs from such adapters show patterns more consistent with L2 processing as observed in humans"

---

### 20. Conclusion - Operationalization Status

**Quoted:** "The operationalization does not reproduce the FLE."

**Why it overreaches:** This conflates (a) the operationalization failing to show the predicted pattern, and (b) the FLE not existing in LLMs. The latter is broader.

**Suggested rewrite:** "This specific operationalization—adapter-prompt language matching—did not produce behavioral patterns consistent with the human FLE."

---

### 21. Conclusion - Comparative Question

**Quoted:** "It also raises a question applicable to the original Costa et al. study: what proportion of human participants gave unclear or invalid responses, and how were these handled?"

**Why it overreaches:** This implies the same measurement is relevant across contexts without establishing comparability.

**Suggested rewrite:** "Our findings on response validity rates raise the question of whether similar metrics were collected in Costa et al. (2014), though the causes of unclear responses likely differ substantially between human and LLM contexts."

---

### 22. Conclusion - Hypothesis Space Claim

**Quoted:** "We publish this negative result to narrow the hypothesis space for future work on computational operationalizations of the Foreign Language Effect in LLMs."

**Why it overreaches:** "Narrow the hypothesis space" implies this rules out a class of approaches, but it only tests one specific implementation.

**Suggested rewrite:** "We publish this negative result as evidence against this particular adapter-based operationalization, informing design choices for future computational investigations of language-dependent decision-making in LLMs."

---

## Summary of Patterns Identified

**Primary issues:**
1. **Mechanism language:** Frequent use of "processing," "interference," "disruption" when only behavioral outputs were measured
2. **Simulation/analogy collapse:** Conflating "we tested X as a proxy for Y" with "X simulates Y"
3. **Operationalization scope:** Claiming results about "the FLE in LLMs" when only one operationalization was tested
4. **Cross-context comparisons:** Implying equivalence between human and LLM response validity issues without justification

**Strengths:**
- Generally good separation of findings (framing effect vs. unclear rates)
- Appropriate hedging in many limitation discussions
- Clear acknowledgment when data is uninterpretable

**Bottom line:** The paper can be substantially strengthened by consistently distinguishing (a) what was measured (choice distributions, response compliance), (b) the specific operationalization tested (adapter-matching), and (c) broader theoretical constructs (L1/L2 processing, the FLE).
