/Users/palaitis/Development/piensa/tools/bin/review-cogsci:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running cogsci reviewer (anthropic/claude-sonnet-4.5)...
## Skeptical Cognitive Science Review: "Piensa Twice"

From a cognitive science perspective, the following points may raise concern about how this paper frames the relationship between human bilingual cognition and LLM behavior:

**1. Foundational Framing Issue: The "Effect" vs. "Processing Mode" Distinction**

The paper treats the Foreign Language Effect as if it were primarily about *which language is used* rather than *how the language is processed*. Costa et al. (2014) attributed the FLE to reduced emotional engagement and increased cognitive distance in L2—qualities arising from developmental history, proficiency asymmetries, and context of acquisition. The paper acknowledges this briefly (Section 5.2) but proceeds as if language identity alone could be operationalized via adapter selection.

This creates a category error: matching adapter language to prompt language tests whether "Spanish processes Spanish prompts differently than it processes English prompts," but the human FLE is about whether *less automatic, less emotionally grounded processing* reduces bias. The paper never establishes that matched adapters are "automatic" or mismatched adapters are "effortful" in any psychologically meaningful sense.

**2. Over-Interpretation of "L1/L2 Mapping"**

The phrase "L1/L2 simulation" appears throughout (Abstract, Section 1.1, Section 5.1), but what is actually being simulated? Human L1 processing involves:
- Implicit emotional associations built over years
- Faster lexical access and reduced cognitive load
- Cultural and experiential grounding of probabilistic language

None of these are present in adapter-prompt matching. The paper acknowledges this in Section 5.2 ("LoRA modifies capabilities, not processing style") but continues to frame the null result as a failure to *reproduce* the FLE rather than a failure to *implement* a mechanism that could plausibly generate it.

**3. The "Unclear Responses" Analogy to Human Data**

The paper's strongest interpretive claim appears in the Abstract and Conclusion: "It also raises a question applicable to the original Costa et al. study: what proportion of human participants gave unclear or invalid responses? The original paper does not report this."

This framing is problematic for several reasons:

- **False equivalence**: Model "unclear responses" are verbose explanations or format violations (Section 4.1). Human unclear responses would be genuine ambivalence, misunderstanding of probabilities, or task non-compliance. These are not the same construct.

- **Implication of methodological laxity**: The phrasing suggests Costa et al. may have failed to report important data. In fact, standard practice in human decision research involves reporting exclusions, attention checks, and comprehension failures. The absence of such a section typically means rates were negligible or standard. Making this "raising a question" appear equivalent to model failures that reach 98% is misleading.

- **Reversal of burden**: The paper's adapters *cause* instruction-following degradation (admitted as a limitation in 5.4), yet the framing redirects attention to whether human researchers reported something they had no reason to expect would be problematic.

**4. Treatment of Alternative Explanations**

The paper briefly mentions that the framing effect "may be too robust" (Section 5.5) and that probability comprehension could matter, but does not seriously engage with these alternatives:

- **Task difficulty**: The Asian Disease Problem requires understanding complementary probabilities (33.3% save all vs. 66.6% save none). Did the models actually process these probabilities, or simply pattern-match "certain" vs. "risky" from surface features? Human studies establish comprehension; this is not tested here.

- **Base rate interpretation**: Humans bring real-world priors about disease, death, and medical intervention. Models have statistical associations from text. Are these commensurable?

- **Demand characteristics**: The instruction "Answer with only 'A' or 'B'" is itself a framing manipulation. Unclear responses might indicate the model detected this and attempted to resist oversimplification—a metacognitive response absent in the human paradigm.

**5. Unexamined Assumptions About Bilingual Processing**

The paper assumes matched adapters should show *stronger* biases (more L1-like, more emotional, less deliberative). But:

- Costa et al. found reduced bias in L2 among balanced bilinguals, not complete elimination
- L1 advantage depends on proficiency, context, and domain
- Some biases (e.g., availability heuristic) might *increase* in L2 if the language provides more salient exemplars

The one-directional prediction (matched→stronger bias) oversimplifies the human literature and makes the null result seem more diagnostic than it is.

**6. Fair Treatment Assessment**

The paper does reasonably well in several areas:
- Section 1.2 summarizes human FLE literature accurately
- The acknowledgment in Section 5.2 that adapters may not create the right kind of processing difference is honest
- The negative result is presented transparently

However, the framing around "unclear responses" and the implicit suggestion that this finding "complicates" the interpretation of Costa et al. crosses into unfair territory. The human study and model evaluation face entirely different validity threats, and presenting them as parallel is a reach.

**7. What Would Make This Comparison Fair?**

A fairer framing would:
- Explicitly state that adapter-language matching is a *surface analogy* to L1/L2, not a mechanistic implementation
- Acknowledge that model instruction-following failures are an artifact of the operationalization, not a property that raises questions about human data quality
- Propose concrete measures of "processing fluency" or "emotional distance" in models (e.g., embedding space geometry, attention entropy) before claiming to test FLE mechanisms

**Conclusion**

The paper makes a reasonable contribution by testing and rejecting a specific operationalization of L1/L2 processing in LLMs. The experimental design is sound and the negative result is valuable. However, the framing sometimes implies that this null finding is *about* the Foreign Language Effect rather than about whether LoRA adapters are the right tool. The suggestion that unclear model responses raise questions about Costa et al.'s human data is the clearest instance of unfair framing, as it equates model engineering failures with potential gaps in human research reporting when no such gap is evidenced.

A cognitive scientist reading this would say: "The paper is careful in its methods but occasionally overreaches in interpreting what the null result means for our understanding of bilingual cognition in humans."
