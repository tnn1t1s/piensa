/Users/palaitis/Development/piensa/tools/bin/review-methods:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running methods reviewer (anthropic/claude-opus-4.5)...
# Methods & Reproducibility Review: Piensa Twice

## Executive Summary

The paper demonstrates strong methodological transparency in most areas, with clear documentation of the experimental design, adapter training, and response classification. However, several aspects require clarification to ensure full reproducibility, and the handling of evaluation failures—while appropriately flagged by the authors—needs more systematic treatment.

---

## Detailed Checklist

### 1. Experimental Design

| Aspect | Status | Notes |
|--------|--------|-------|
| Factorial structure (4×4×2) | **Clear** | Well-specified: adapter × prompt × frame |
| Trial count per condition | **Clear** | 50 trials × 32 conditions = 1,600 total |
| Randomization/ordering | **Missing** | Were trials run in fixed or randomized order? Could order effects exist? |
| Independence of trials | **Needs clarification** | At temperature=0.7, are trials truly independent or is there session/context carryover? |

**Interpretability risk**: If trials were run sequentially in a single session per condition, context window effects could introduce dependencies.

---

### 2. Adapter Training Details

| Aspect | Status | Notes |
|--------|--------|-------|
| LoRA hyperparameters | **Clear** | Rank=8, Alpha=16, LR=1e-5, iterations=100, batch=2 |
| Target layers specification | **Needs clarification** | "Target layers: 16"—which 16 layers? All attention? Q/K/V/O projections? |
| Training data sources | **Clear** | All four sources identified with sample counts |
| Validation loss reporting | **Clear** | Final losses provided per adapter |
| Training/validation split | **Missing** | What proportion was held out for validation? |
| Checkpoint selection | **Missing** | Was final checkpoint used, or best validation loss? |
| Training hardware/time | **Missing** | Relevant for reproducibility |

**Interpretability risk**: The Hebrew adapter used GPT-4o translation while others used native corpora. This introduces a confound: Hebrew results may reflect translation artifacts rather than language properties. The authors note this in limitations but it affects comparability.

---

### 3. Inference Parameters

| Aspect | Status | Notes |
|--------|--------|-------|
| Temperature | **Clear** | 0.7 specified |
| Max tokens | **Missing** | Critical for understanding truncation behavior |
| Top-p/top-k | **Missing** | Default values should be stated |
| System prompt | **Missing** | Was one used? Could affect instruction-following |
| Quantization details | **Partial** | "4-bit via MLX" stated, but specific quantization scheme (e.g., GPTQ, AWQ, native MLX) not specified |
| Adapter loading method | **Missing** | How were adapters applied at inference? |

**Interpretability risk**: Missing max_tokens is particularly concerning—if set too low, it could cause truncation that gets classified as "unclear"; if unlimited, it permits verbose responses.

---

### 4. Response Classification Rules

| Aspect | Status | Notes |
|--------|--------|-------|
| Classification criteria | **Clear** | Three-way: A, B, Unclear with explicit rules |
| Edge case handling | **Needs clarification** | "starts with 'A' with no mention of 'B'"—what about "A, because..." responses? |
| Case sensitivity | **Missing** | Were lowercase a/b accepted? |
| Language-specific variants | **Missing** | Were Hebrew א/ב or Chinese equivalents checked? |
| Classification automation | **Missing** | Manual, regex, or LLM-based classification? |

**Specific concern**: The rule "starts with 'A' with no mention of 'B'" would classify "A. Medicine A is better because Medicine B has risk..." as unclear (mentions B), but this seems like a valid A response. The Type 2 examples (Spanish adapter) show responses starting with the correct letter but continuing—how were these actually classified?

**Recommended clarification**: Provide the exact classification code or rubric, with worked examples of edge cases.

---

### 5. Unclear Rate Handling

| Aspect | Status | Notes |
|--------|--------|-------|
| Unclear rate reporting | **Clear** | Reported per condition in Table 1 |
| Interpretation threshold | **Clear** | Authors flag >50% as limiting interpretability |
| Denominator calculation | **Needs clarification** | Is Δ calculated over all 50 trials or only clear responses? |
| Statistical treatment | **Missing** | No confidence intervals or significance tests reported |

**Critical ambiguity**: The formula "P(A|gain) - P(A|loss)" does not specify whether unclear responses are:
- (a) Excluded from the denominator (probabilities over clear responses only)
- (b) Included in denominator (probabilities over all trials)
- (c) Treated as a third category in multinomial analysis

This fundamentally affects interpretation. For example, in ES+ES (loss frame) with 68% unclear:
- If (a): P(A|loss) = 12%/(12%+20%) = 37.5% of clear responses
- If (b): P(A|loss) = 12% of all trials

The reported values appear consistent with option (b), but this should be explicit.

---

### 6. Prompt Translation and Equivalence

| Aspect | Status | Notes |
|--------|--------|-------|
| Full prompts provided | **Clear** | All four gain-frame versions shown |
| Loss frame prompts | **Missing** | Only English loss frame shown; others should be in appendix |
| Number formatting | **Partial** | Spanish uses periods (600.000), others use commas—could affect parsing |
| Translation validation | **Missing** | Were translations verified by native speakers? |

**Interpretability risk**: Number formatting differences (600.000 vs 600,000) could interact with tokenization differently across languages.

---

### 7. Statistical Analysis

| Aspect | Status | Notes |
|--------|--------|-------|
| Confidence intervals | **Missing** | With n=50 per condition, binomial CIs should be reported |
| Significance testing | **Missing** | No formal comparison of matched vs. mismatched |
| Multiple comparisons | **Not addressed** | 16 conditions × 2 frames = 32 comparisons |
| Power analysis | **Missing** | Was n=50 sufficient to detect expected effect sizes? |

**Interpretability risk**: The claim that EN-EN (54%) shows "weaker" framing than EN-ES (70%) or EN-HE (76%) is stated without uncertainty quantification. With n=50 and temperature-induced variance, these differences may not be statistically meaningful.

**Quick calculation**: For EN-EN gain (100% of 50 = 50/50) vs. loss (46% = 23/50), the framing effect 95% CI is approximately [40%, 68%] using Wilson score intervals. For EN-ES (Δ=70%), the CI overlaps substantially. The "opposite of prediction" claim needs statistical support.

---

### 8. Reproducibility Materials

| Aspect | Status | Notes |
|--------|--------|-------|
| Code availability | **Missing** | No repository or code mentioned |
| Data availability | **Missing** | Raw response data not mentioned |
| Trained adapters | **Missing** | Will these be released? |
| Random seeds | **Missing** | Not specified |

---

## Summary of Required Clarifications for Reproducibility

### Must-fix (affects interpretation):
1. **Δ calculation denominator**: Explicit formula with unclear handling
2. **Max tokens parameter**: Critical for understanding truncation vs. verbosity
3. **Target layers specification**: Which specific layers received LoRA adaptation
4. **Classification edge cases**: Handling of "A. [explanation mentioning B]" responses
5. **Confidence intervals**: At minimum for the key EN adapter comparisons

### Should-fix (affects replication):
6. Trial ordering and independence
7. Full loss-frame prompts in all languages
8. Training/validation split and checkpoint selection
9. System prompt (if any)
10. Exact quantization scheme

### Nice-to-have:
11. Code/data repository
12. Random seeds
13. Hardware specifications

---

## Assessment of Evaluation Failure Handling

The authors appropriately identify instruction-following degradation as a central finding and flag conditions with >50% unclear rates as uninterpretable. This is methodologically sound. However:

**Strength**: The paper treats unclear rates as a first-class outcome rather than hiding or excluding them.

**Gap**: The paper doesn't analyze whether unclear responses are *informative*—do they correlate with any choice preference? A response like "B. Si elige el Medicamento B..." is classified as unclear but clearly indicates B preference. The taxonomy in Section 4.1 distinguishes response types but doesn't quantify them or test whether "recoverable" unclear responses would change conclusions.

**Recommendation**: Report what proportion of unclear responses contain recoverable choice information, and conduct sensitivity analysis including these.

---

## Overall Assessment

**Reproducibility Grade: B-**

A reader with access to the same base model and training data could approximately replicate this experiment, but several missing details (max_tokens, exact classification code, target layers, Δ calculation) could lead to different results. The statistical interpretation is under-supported.

**Interpretability Grade: B+**

The authors are admirably transparent about limitations and clearly flag which results are interpretable. The main gap is the missing uncertainty quantification for the key comparative claims.

**Key methodological contribution confirmed**: The documentation of instruction-following degradation as a function of adapter-prompt mismatch is well-supported and replicable from the provided information.
