#!/usr/bin/env python3
"""
Submit a fine-tuning job to OpenAI.

Usage:
    openai-finetune --data train.jsonl --model gpt-4o --epochs 3
    openai-finetune --data train.jsonl --model gpt-4o-mini --suffix old-birds

Output: JSON with job info to stdout
Logging: Progress to stderr with flush
"""

import argparse
import hashlib
import json
import os
import sys
import time
from pathlib import Path


def compute_hash(path: Path) -> str:
    """Compute sha256 hash of file."""
    sha256 = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest()

from dotenv import load_dotenv
load_dotenv()

try:
    from openai import OpenAI
except ImportError:
    print("[openai-finetune] ERROR: openai package not installed. Run: pip install openai", file=sys.stderr)
    sys.exit(1)


def log(msg: str):
    """Log to stderr with flush."""
    print(msg, file=sys.stderr, flush=True)


def main():
    parser = argparse.ArgumentParser(description="Submit fine-tuning job to OpenAI")
    parser.add_argument("--data", "-d", required=True, help="Training data JSONL file")
    parser.add_argument("--model", "-m", default="gpt-4o", help="Base model to fine-tune")
    parser.add_argument("--epochs", "-e", type=int, default=3, help="Number of epochs")
    parser.add_argument("--batch-size", "-b", type=int, default=1, help="Batch size")
    parser.add_argument("--learning-rate", "-l", type=float, help="Learning rate multiplier (default: auto)")
    parser.add_argument("--suffix", "-s", help="Model suffix for identification")
    parser.add_argument("--wait", "-w", action="store_true", help="Wait for job to complete")
    args = parser.parse_args()

    # Check data file
    data_path = Path(args.data)
    if not data_path.exists():
        log(f"[openai-finetune] ERROR: Data file not found: {args.data}")
        sys.exit(1)

    # Count samples and compute hash for provenance
    with open(data_path) as f:
        sample_count = sum(1 for _ in f)
    dataset_hash = compute_hash(data_path)
    log(f"[openai-finetune] data={args.data} samples={sample_count} sha256={dataset_hash[:16]}...")

    # Initialize client
    client = OpenAI()

    # Upload training file
    log(f"[openai-finetune] uploading training file...")
    with open(data_path, "rb") as f:
        file_response = client.files.create(file=f, purpose="fine-tune")
    file_id = file_response.id
    log(f"[openai-finetune] uploaded file_id={file_id}")

    # Create fine-tuning job
    log(f"[openai-finetune] creating job: model={args.model} epochs={args.epochs} batch_size={args.batch_size}")

    hyperparameters = {
        "n_epochs": args.epochs,
        "batch_size": args.batch_size,
    }
    if args.learning_rate:
        hyperparameters["learning_rate_multiplier"] = args.learning_rate

    job_params = {
        "training_file": file_id,
        "model": args.model,
        "hyperparameters": hyperparameters,
    }
    if args.suffix:
        job_params["suffix"] = args.suffix

    job = client.fine_tuning.jobs.create(**job_params)
    log(f"[openai-finetune] job created: id={job.id} status={job.status}")

    if args.wait:
        log(f"[openai-finetune] waiting for job to complete...")
        while True:
            job = client.fine_tuning.jobs.retrieve(job.id)
            log(f"[openai-finetune] status={job.status}")
            if job.status in ["succeeded", "failed", "cancelled"]:
                break
            time.sleep(30)

        if job.status == "succeeded":
            log(f"[openai-finetune] SUCCESS: fine_tuned_model={job.fine_tuned_model}")
        else:
            log(f"[openai-finetune] FAILED: status={job.status}")
            if job.error:
                log(f"[openai-finetune] error: {job.error}")

    # Output job info with full provenance
    output = {
        "job_id": job.id,
        "status": job.status,
        "model": args.model,
        "training_file": file_id,
        "dataset_hash": dataset_hash,
        "sample_count": sample_count,
        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "learning_rate_multiplier": args.learning_rate,
        "fine_tuned_model": job.fine_tuned_model,
    }
    print(json.dumps(output, indent=2))


if __name__ == "__main__":
    main()
