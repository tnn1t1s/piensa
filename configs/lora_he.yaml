# Hebrew LoRA Adapter Configuration
# Inherits from base.yaml

_base_: "base.yaml"

adapter:
  name: "lora_he"
  language: "he"

dataset:
  # DictaLM uses a different format - may need custom loader
  name: "dicta-il/dictalm2.0-instruct"
  subset: null
  split: "train"
  # Note: DictaLM is already instruction-tuned on Mistral
  # We may want to use their training data directly or
  # create a Hebrew Alpaca translation for better comparability

output_dir: "adapters/lora_he"
