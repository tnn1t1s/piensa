/Users/palaitis/Development/piensa/tools/bin/review-cogsci:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running cogsci reviewer (openai/gpt-5.1)...
From a cognitive science perspective, the following points may raise concern about interpretive framing and the way the human literature is used as a reference class:

1. **Over-strong mapping from LoRA alignment to “L1/L2”**  
   The central analogy—matched adapter–prompt = L1, mismatched = L2—is extremely thin from a bilingual cognition standpoint. Human L1/L2 differences involve:
   - developmental history,
   - proficiency,
   - emotional grounding,
   - context of acquisition,
   - and use patterns over time.  
   LoRA adapters here are essentially narrow, recent fine-tuning regimes on top of an overwhelmingly English-trained base model. That is closer to adding or suppressing surface capabilities than to shifting a system between “native” and “foreign” processing modes.  
   The paper does acknowledge in §5.2 that LoRA “modifies capabilities, not processing style,” but much of the framing (e.g., “L1 simulation”, “L2 simulation”, “testable operationalization of L1/L2”) risks inviting readers to treat this as a meaningful cognitive analogue. A more guarded phrasing—“a very crude engineering proxy inspired by L1/L2 asymmetries”—would better reflect how distant this setup is from the constructs Costa et al. studied.

2. **Characterization of the human Foreign Language Effect (FLE)**  
   The description of the FLE as arising from “emotional distance” that “promotes more deliberative, ‘System 2’ reasoning” reflects one influential interpretation, but the human literature is more complex:
   - Effects depend strongly on proficiency, immersion, and task type; in some domains FLE is small or absent.
   - Competing or complementary accounts emphasize changes in pragmatic interpretation, concreteness, imagery, and reliance on verbatim vs. gist representations, not just “more System 2.”  
   You do cite caveats about heterogeneity via meta-analyses, but the theoretical summary in §1.1–1.2 can give an impression of a single dominant mechanism that maps straightforwardly onto engineering interventions. For a reader steeped in Costa et al. and successors, that feels somewhat oversimplified.

3. **Human “response validity” is not well-aligned with LLM “unclear” responses**  
   A substantial part of your contribution is to treat non-compliant LLM outputs as a first-class outcome. That is valuable for model evaluation, but the analogy you draw to human data quality is problematic.  
   - In human experiments, someone can perfectly well choose A or B and also provide an explanation or verbal reflection; the choice is easily coded regardless of verbosity.  
   - Your “unclear” category often consists of exactly that kind of explanation (Type 1/2/3), which is only “invalid” because the LLM was asked to output a single character.  
   The conclusion section then asks, “what proportion of human participants gave unclear or invalid responses? The original paper does not report this.” This reads as if the Costa et al. methodology might be systematically under-reporting an issue analogous to your non-compliance problem. For humans, however:
   - The task is simple forced choice with very minimal format demands.
   - Any exclusions (e.g., non-responses, failed comprehension checks) are categorically different from verbose but perfectly categorizable answers.  
   To avoid unfairly casting doubt on Costa et al., it would be better to explicitly note that your “response validity” problem is a model–prompt artifact and not directly parallel to typical human data screening procedures.

4. **Equating surface-level framing patterns with “the same” bias as in humans**  
   You rightly acknowledge earlier work showing that LLMs can reproduce the Asian Disease framing pattern. However, several phrases—e.g., “the framing effect is real (where measurable)”—risk suggesting that the model exhibits the same cognitive phenomenon humans do.  
   From a cognitive science standpoint:
   - Similar choice proportions under different phrasings are, at best, a necessary but not sufficient indicator of a shared underlying mechanism.
   - In your own study, the same setup that yields a clear Δ in some language conditions also collapses into format non-compliance in others, underscoring how far we are from stable “cognitive” behavior.  
   You do note elsewhere that LLM multilingual behavior may reflect “surface-level variation, decoding artifacts, or instruction-following instability,” but that caution could be kept more front-and-center whenever the model’s bias is compared to human biases.

5. **Use of the negative result with respect to human FLE**  
   The paper is careful to phrase its main finding as a negative result for *this operationalization* of the FLE in LLMs. That is appropriate. However, there are places where the rhetoric could be read as more general than warranted:
   - Statements such as “We find that the operationalization of L1/L2 processing tested here […] does not reproduce the Foreign Language Effect reported in human studies” are technically correct but may be interpreted—especially by non-specialists—as “we tested the FLE in LLMs and it isn’t there.”  
   Given how distant your adapter-based manipulation is from core human FLE constructs, and how dominant the instruction-following artifact turned out to be, it would be fairer to stress that:
   - The result primarily constrains *this very specific engineering proxy*,  
   - and says little about the plausibility or robustness of the human FLE itself.

6. **Alternative explanations for both human and LLM behavior**  
   You briefly mention that human tasks rely on “probabilistic statements that may be cognitively demanding,” but this is only used to motivate a question about unreported invalid responses. There is a richer set of considerations that could be acknowledged without undermining Costa et al.:
   - For humans, the FLE may interact with conceptual and probabilistic understanding (e.g., differences in how numerical information is encoded in L1 vs. L2, or in how risk is linguistically framed).
   - For LLMs, differences across adapters and languages are plausibly dominated by instruction-tuning imbalances, tokenization issues, or training-data artifacts, not by anything akin to “emotional distance.”  
   Being explicit that your observed patterns are almost certainly downstream of these model-specific factors would help prevent over-interpretation of the comparison with human bilinguals.

Overall, the paper is commendably explicit about its negative result and cautious about claims of mechanism. The main improvements, from a cognitive science standpoint, would be:
- softening the strength of the L1/L2 analogy,
- avoiding implications that human studies may be hiding comparable “invalid response” issues, and
- reiterating more often that similar choice distributions do not, by themselves, license strong analogies to human cognitive biases or to theoretical accounts like those in Costa et al.
