/Users/palaitis/Development/piensa/tools/bin/review-epistemic:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running epistemic reviewer (anthropic/claude-opus-4.5)...
# Epistemic Hygiene Review: Claim-Scope Audit

## Executive Summary

This paper is notably well-scoped compared to typical work in this area. The authors consistently frame findings as specific to their operationalization and acknowledge limitations. However, several sentences still overreach beyond what was tested. Below are specific instances requiring scope-narrowing.

---

## Flagged Claims

### 1. Abstract

**Quoted text:** "we confirm that Mistral-7B exhibits the classic framing effect"

**Why it overreaches:** "Confirm" implies verification of a general property of Mistral-7B. The experiment tested one framing scenario (Asian Disease) with one prompt format, one temperature setting, and 4-bit quantization. The model may not exhibit this effect under other framing paradigms, temperatures, or quantization levels.

**Suggested rewrite:** "we observe that Mistral-7B exhibits the classic framing effect in this task configuration"

---

### 2. Section 1.1

**Quoted text:** "The theoretical explanation centers on emotional distance: L2 processing is more effortful and less emotionally resonant than L1 processing, which promotes more deliberative, 'System 2' reasoning"

**Why it overreaches:** Presented as established fact rather than as one proposed mechanism among several. The paper later acknowledges alternative accounts exist (Section 1.2 mentions "reduced emotional engagement... rather than increased analytical reasoning capacity"), but this sentence states the dual-process account without qualification.

**Suggested rewrite:** "A prominent theoretical explanation proposes that L2 processing is more effortful and less emotionally resonant than L1 processing, which may promote more deliberative reasoning"

---

### 3. Section 1.2

**Quoted text:** "These findings suggest that aggregate choice patterns in LLMs can resemble those observed in human decision-making."

**Why it overreaches:** "Resemble" implies similarity beyond surface-level output matching. The cited studies measured choice distributions; whether the underlying processes bear any resemblance is not established by behavioral correspondence.

**Suggested rewrite:** "These findings indicate that aggregate choice distributions in LLMs can parallel those observed in human experiments using the same paradigms."

---

### 4. Section 3.1

**Quoted text:** "confirming that Mistral-7B displays the classic Tversky-Kahneman bias pattern across languages"

**Why it overreaches:** "Across languages" suggests comprehensive multilingual coverage. The experiment tested four languages, several of which had majority-unclear response rates. The framing effect was only cleanly observable in conditions with low unclear rates.

**Suggested rewrite:** "indicating that Mistral-7B displays the classic Tversky-Kahneman bias pattern in the conditions where responses were interpretable"

---

### 5. Section 4.3

**Quoted text:** "language-specific LoRA training may disrupt the instruction-following behavior learned during the base model's instruction tuning phase"

**Why it overreaches:** This is appropriately hedged with "may," but the causal claim ("disrupt... learned during") implies a specific mechanism. The experiment observed correlation between non-English adapters and unclear responses but did not isolate whether the disruption occurs to instruction-following specifically, or whether it reflects other factors (e.g., tokenization mismatches, training data quality differences).

**Suggested rewrite:** "language-specific LoRA training is associated with degraded instruction-following, though whether this reflects disruption of learned behavior or other factors (e.g., training data characteristics, tokenization) remains undetermined"

---

### 6. Section 5.1

**Quoted text:** "monolingual LoRA fine-tuning can fragment multilingual capabilities"

**Why it overreaches:** "Can fragment" asserts a general capability of monolingual LoRA. The experiment tested one base model, one LoRA configuration, one set of training data sources. The fragmentation may be specific to these choices.

**Suggested rewrite:** "in this configuration, monolingual LoRA fine-tuning fragmented multilingual capabilities"

---

### 7. Section 5.2

**Quoted text:** "Fine-tuning on Spanish text may make the model better at generating Spanish without creating any asymmetry in cognitive engagement."

**Why it overreaches:** "Cognitive engagement" anthropomorphizes model processing without operational definition. The experiment measured choice distributions and response validity, not anything that could be mapped to "engagement."

**Suggested rewrite:** "Fine-tuning on Spanish text may improve Spanish generation without creating processing asymmetries detectable in choice behavior."

---

### 8. Section 5.3

**Quoted text:** "The framing effect Is Real (Where Measurable)"

**Why it overreaches:** Section heading uses "Real" which implies ontological status beyond what was measured. The experiment detected a statistical pattern in choice distributions.

**Suggested rewrite:** "The Framing Effect Is Observed (Where Measurable)" or "Framing Effects Replicate (Where Measurable)"

---

### 9. Section 5.3

**Quoted text:** "the framing effect was robust"

**Why it overreaches:** "Robust" implies stability across perturbations not tested (different temperatures, prompt phrasings, model sizes, etc.). The experiment used fixed parameters.

**Suggested rewrite:** "the framing effect was consistently observed in these conditions"

---

### 10. Section 6 (Conclusion)

**Quoted text:** "This work contributes a negative result."

**Why it overreaches:** Minor issue, but "negative result" implies the hypothesis was adequately tested and disconfirmed. Given that instruction-following degradation prevented clean measurement in most conditions, the result is partially "inconclusive for methodological reasons" rather than purely "negative."

**Suggested rewrite:** "This work contributes a negative result for the conditions where measurement was possible, and documents methodological barriers that prevented clean testing in other conditions."

---

### 11. Section 6

**Quoted text:** "the operationalization of L1/L2 processing tested here... does not reproduce the Foreign Language Effect reported in human studies"

**Why it overreaches:** Appropriately scoped to "the operationalization tested here," but "does not reproduce" is strong given that only EN-adapter conditions had clean data. For other adapters, the high unclear rates prevent confident assessment of whether the FLE was absent or merely unmeasurable.

**Suggested rewrite:** "the operationalization of L1/L2 processing tested here—in the conditions where response data was interpretable—does not reproduce the Foreign Language Effect reported in human studies"

---

## Items Correctly Scoped (No Changes Needed)

The following demonstrate good epistemic hygiene:

- "We tested whether language-specific LoRA adapters could operationalize this Foreign Language Effect" (Abstract) — appropriately frames as testing an operationalization, not the phenomenon itself
- "the operationalization tested here does not reproduce the predicted L1/L2 pattern" (Abstract) — correctly scoped
- "This evaluation failure is distinct from the proxy failure" (Abstract) — important distinction maintained throughout
- "Within this experimental setup, that assumption does not appear to hold" (Section 5.2) — appropriately bounded
- "Results may differ for other architectures or precisions" (Section 5.4) — explicit limitation

---

## Structural Observation

The paper commendably maintains separation between:
1. What was operationalized (adapter-prompt matching as L1/L2 proxy)
2. What was measured (choice distributions, unclear rates)
3. What the target phenomenon is (human FLE)

This three-way distinction prevents most forms of overreach. The flagged items above are relatively minor compared to typical papers in this space.
