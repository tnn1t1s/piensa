/Users/palaitis/Development/piensa/tools/bin/review-methods:46: DeprecationWarning: `OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.
  model = OpenAIModel(model_name, provider='openrouter')
Running methods reviewer (anthropic/claude-sonnet-4.5)...
# Methods & Reproducibility Review: "Piensa Twice"

## EXPERIMENTAL DESIGN CLARITY

### Core Design: **Clear**
- 4×4×2 factorial (adapter × prompt × frame) = 32 conditions
- 50 trials per condition at temperature=0.7
- Matched vs. mismatched adapter-prompt logic is explicit

### Operationalization Concern: **Needs Clarification**
**Issue**: The L1/L2 proxy is weakly justified a priori.
- The paper states LoRA adapters "operationalize" native vs. foreign processing, but provides no theoretical mechanism for *why* adapter-prompt matching would create processing asymmetry analogous to human L1/L2.
- Section 5.2 acknowledges this retrospectively ("LoRA modifies capabilities, not processing style"), but this undermines the experimental premise.

**Interpretability Risk**: A reader might not realize the operationalization was speculative until reaching the discussion. The hypothesis section should explicitly flag this as exploratory.

---

## ADAPTER TRAINING DETAILS

### Reported: **Clear**
- Rank=8, Alpha=16, 16 target layers, lr=1e-5, 100 iterations, batch size=2
- Dataset sources and sizes are documented (Table in 2.2)
- Validation loss provided for each adapter

### Missing: **Critical Details for Replication**
1. **Which specific layers?** "Target layers: 16" is ambiguous. Does this mean 16 layers were targeted, or layer 16 only? Which layers (attention, FFN, both)?
2. **Optimizer?** Adam? AdamW? SGD?
3. **Dropout or regularization?** Not specified.
4. **Training duration justification?** Why 100 iterations? Was convergence checked?
5. **Quantization impact?** 4-bit quantization applied *before* or *after* LoRA training? Does quantization interact with adapter effectiveness?
6. **Base model instruction-tuning checkpoint?** Mistral-7B-Instruct-v0.3 presumably already instruction-tuned—does LoRA overwrite this?

**Interpretability Risk**: Without these details, unclear why ES and ZH adapters degraded instruction-following while EN did not. Was it a training hyperparameter issue or inherent to the approach?

---

## INFERENCE PARAMETERS

### Reported: **Clear**
- Temperature = 0.7
- 50 independent trials per condition

### Missing: **Could Affect Results**
1. **Top-p or top-k sampling?** Not mentioned.
2. **Max tokens?** If the model was allowed arbitrary generation length, this could explain verbose responses.
3. **Stop tokens?** Were any specified?
4. **Seed control?** Were trials truly independent or sequential from a single seed?

**Interpretability Risk**: High unclear rates in some conditions might be artifacts of permissive decoding rather than adapter effects.

---

## RESPONSE CLASSIFICATION RULES

### Reported: **Adequate but Loose**
- "A": Contains only "A" or starts with "A" with no mention of "B"
- "B": Contains only "B" or starts with "B" with no mention of "A"
- "Unclear": Both mentioned, neither mentioned, or verbose

### Ambiguities:
1. **Case sensitivity?** "a" vs. "A"?
2. **Punctuation handling?** Is "A." different from "A "?
3. **Edge case**: "A is better but I choose B"—which rule triggers first?
4. **Language mixing**: If a response is "B. Si elige..." (Type 2 example), why is this "unclear" rather than "B"? The rule says "starts with B with no mention of A"—this fits.

**Interpretability Risk**: The Type 2 examples in Section 4.1 suggest classification may have been stricter than stated rules imply. Manual review may have introduced subjectivity.

---

## UNCLEAR-RATE HANDLING

### Strength: **Excellent Transparency**
This is the paper's major methodological contribution. The authors:
- Report unclear rates alongside framing effects (Table 1)
- Flag when data becomes uninterpretable (>50% unclear)
- Provide taxonomy of failure modes (Section 4.1)
- Explicitly state "Framing effects should be interpreted with caution when unclear rates exceed 50%"

### Critical Observation on Human Study:
The paper notes Costa et al. (2014) **do not report unclear/invalid response rates**. This is a significant methodological point:
- If human participants also produced unclear responses differentially across L1/L2 conditions, failure handling could confound the original FLE findings.
- Standard practice in behavioral economics is to report exclusion criteria and rates.

**This deserves emphasis in revision**: The paper should explicitly state whether Costa et al. reported:
- Comprehension check failures
- Non-responses
- Invalid or ambiguous answers
- Differential exclusion rates by language condition

---

## BIAS RISKS

### Confound: **Acknowledged but Unresolved**
The instruction-following degradation is correctly identified as a confound, but the paper does not:
1. **Test whether unclear responses are random** or systematically biased toward one option.
2. **Report whether unclear responses contained extractable preferences** (e.g., "I prefer A because..." is unclear by format but contains a choice).

**Interpretability Risk**: If unclear responses in the ZH adapter systematically lean toward the risky option, excluding them would *artificially inflate* the framing effect. The paper cannot rule this out.

### Selection Bias:
By excluding unclear responses from P(A|gain) and P(A|loss) calculations, the reported framing effects represent **conditional distributions** (framing effect *given the model provides a clear answer*). This is not the same as the unconditional framing effect.

For ZH+EN (96% unclear), the 4% clear responses may be unrepresentative outliers.

---

## EVALUATION FAILURE MODES

### Well-Documented:
The taxonomy (Type 1-3) is valuable:
- Type 1 (format non-compliance): model explains instead of answering
- Type 2 (language leakage): starts correctly but continues in wrong language
- Type 3 (instruction override): philosophical reflection instead of choice

### Underexplored:
1. **Are these failure modes correlated with framing condition?** If gain frames produce more Type 1 errors, this could bias Δ estimates.
2. **Could unclear responses be extracted with post-processing?** Example: "B. Si elige el Medicamento B..." contains "B"—why not extract it?

**Recommendation**: Report unclear rates separately for gain vs. loss frames (currently only combined). If ES adapter shows 32% unclear in gain but 68% in loss, the framing effect estimate is unreliable even before calculating Δ.

---

## MISSING CONTROLS

### No Adapter Baseline:
The paper does not include a **base model (no adapter) × 4 prompt languages** condition. This would disambiguate:
- Whether framing effects in EN+EN come from the adapter or the base model
- Whether unclear rates are caused by adapter training or pre-existing multilingual weaknesses

### No Cross-Lingual Prompts:
The Asian Disease problem translations are not back-translated or validated for equivalence. Differences in framing magnitude could reflect translation artifacts (e.g., "morirán" may carry different emotional weight than "will die").

---

## REPRODUCIBILITY BLOCKERS

### High Priority:
1. **Adapter checkpoints not shared** (stated intention to "publish" but not confirmed)
2. **Exact prompt templates** provided only for gain frames; loss frame examples given but full text not in supplementary
3. **Classification code** not provided—manual review process not auditable
4. **Random seed** not specified

### Medium Priority:
5. Mistral-7B-Instruct-v0.3 checkpoint hash/URL
6. MLX version and quantization config
7. Exact number of trials per condition (stated as 50—were any conditions rerun?)

**Recommendation**: Provide a reproducibility appendix with:
- Full prompts for all 8 conditions (4 languages × 2 frames)
- Adapter training script
- Response classification regex or code
- Raw response CSV with trial-level data

---

## STATISTICAL RIGOR

### Adequate for Exploratory Study:
- 50 trials per condition is reasonable
- Framing effects are large (30-88%) relative to binomial sampling error (~14% at n=50, p=0.5)

### Missing:
1. **No confidence intervals** on Δ estimates
2. **No significance tests** (though given the effect sizes, most would be significant)
3. **No correction for multiple comparisons** (16 adapter-prompt pairs × 2 frames = 32 tests)

**Not a blocker** for a negative result paper, but limits precision of claims like "ES: 70% > EN: 54%"—is this difference meaningful or noise?

---

## REPLICATION FEASIBILITY

### Can a reader re-run the experiment?
**Partially.**

**What's reproducible**:
- Mistral-7B-Instruct-v0.3 is public
- LoRA config is specified
- Training datasets are identified

**What's not reproducible without author assistance**:
- Exact adapter weights (need checkpoint files)
- Classification decisions (manual review component)
- Hebrew translation (GPT-4o prompt not provided)

**Estimated effort**: With released adapters and code: ~1 week. Without: ~1 month.

---

## INTERPRETABILITY OF RESULTS

### What can be concluded:
1. **EN adapter shows framing effect across all prompt languages** (0-2% unclear rates)
2. **Matched ≠ stronger framing** in the one clean test case (EN+EN: 54% vs. EN+HE: 76%)
3. **ZH adapter catastrophically fails instruction-following**

### What cannot be concluded:
1. **Whether other adapters show framing effects** (unclear rates too high)
2. **Whether mismatched conditions truly simulate L2** (operationalization not validated)
3. **Whether the result generalizes** (one model, one task, one framing paradigm)

---

## STRENGTHS

1. **Honest reporting of failure modes**: The unclear rate documentation is exemplary
2. **Transparent negative result**: Does not oversell findings
3. **Clear research questions**: RQ1-3 are answerable
4. **Thoughtful discussion**: Section 5.5 proposes better alternatives

---

## CRITICAL GAPS

### Tier 1 (Blocks Replication):
- [ ] Specify which transformer layers were targeted by LoRA
- [ ] Report optimizer and regularization
- [ ] Provide adapter checkpoint files or training scripts
- [ ] Clarify response classification procedure (manual vs. automated)

### Tier 2 (Limits Interpretation):
- [ ] Report unclear rates separately for gain vs. loss frames
- [ ] Test whether unclear responses contain extractable preferences
- [ ] Include base model (no adapter) baseline
- [ ] Provide confidence intervals on Δ estimates

### Tier 3 (Strengthens Claims):
- [ ] Back-translate prompts to verify equivalence
- [ ] Test alternative decoding parameters (temperature, max_tokens)
- [ ] Validate that low-unclear conditions are not cherry-picked

---

## VERDICT

### Reproducibility: **Partial**
- Core design is clear
- Training details have critical gaps
- Inference setup underspecified
- Raw data not provided

### Interpretability: **Good with Caveats**
- The paper correctly identifies which results are interpretable (EN adapter) and which are not (ZH adapter)
- The unclear rate reporting is a model for future work
- The negative result is scientifically valid even if not mechanistically explained

### Recommendations for Revision:

1. **Add supplementary materials**:
   - Full prompts (all 8 language-frame combinations)
   - Adapter training script
   - Response classification code
   - Trial-level raw data

2. **Clarify LoRA configuration**:
   - Which layers (by name/index)
   - Optimizer and regularization

3. **Strengthen Costa et al. comparison**:
   - Explicitly state whether original study reported unclear/invalid response rates
   - If not, note this as a limitation of the human literature

4. **Add base model baseline**:
   - Test whether framing effects and unclear rates exist without adapters

5. **Report frame-specific unclear rates**:
   - Current Table 1 aggregates gain/loss—separate these

---

## FINAL ASSESSMENT

**This paper is methodologically honest and identifies real evaluation challenges in adapting human cognitive paradigms to LLMs.** The instruction-following degradation finding is valuable and reproducible (within the limits noted above).

**However, the adapter training and inference procedures have sufficient ambiguity that exact replication would require author assistance.** Key missing details: layer specification, optimizer, decoding parameters, and response classification procedure.

**The interpretability risks are well-managed**: the paper correctly flags when data becomes uninterpretable and does not over-claim. The negative result is scientifically sound, though limited to this specific operationalization.

**Success criterion met?** A reader could design a similar experiment, but could not exactly replicate this one without additional information.

---

**Token budget used: ~2,800 / 1,000,000**
